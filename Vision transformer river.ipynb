{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nachikaet/River-water-level--LSTM/blob/main/Vision%20transformer%20river.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full corrected script — uses your filenames & credentials and re-authenticates each loop.\n",
        "\n",
        "!pip install rasterio geopandas shapely transformers torch torchvision tqdm requests --quiet\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "import rasterio\n",
        "import cv2\n",
        "from shapely.geometry import Polygon\n",
        "from transformers import AutoFeatureExtractor, AutoModel\n",
        "\n",
        "# -------------------------\n",
        "# Your Copernicus credentials (as provided by you)\n",
        "# -------------------------\n",
        "CATALOG_URL = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products\"\n",
        "AUTH_URL = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
        "USERNAME = \"nachikaet10l20@gmail.com\"\n",
        "PASSWORD = \"Sigmaskibidi2005*\"\n",
        "\n",
        "# -------------------------\n",
        "# Drive paths (as you used)\n",
        "# -------------------------\n",
        "DRIVE_CSV_IN = \"/content/drive/My Drive/ML/finalLSTM.csv\"\n",
        "DRIVE_OUTPUT = \"/content/drive/My Drive/ML/extracted_features_with_dates.csv\"\n",
        "\n",
        "# local working dir for downloads/extract\n",
        "WORK_DIR = \"data\"\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Model / extractor (same as your original)\n",
        "# -------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model.eval().to(device)\n",
        "\n",
        "# -------------------------\n",
        "# Authentication helper (same flow you used)\n",
        "# -------------------------\n",
        "def authenticate():\n",
        "    data = {\n",
        "        \"client_id\": \"cdse-public\",\n",
        "        \"grant_type\": \"password\",\n",
        "        \"username\": USERNAME,\n",
        "        \"password\": PASSWORD\n",
        "    }\n",
        "    r = requests.post(AUTH_URL, data=data, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    tok = r.json().get(\"access_token\")\n",
        "    if not tok:\n",
        "        raise RuntimeError(\"Authentication returned no access_token\")\n",
        "    return tok\n",
        "\n",
        "# -------------------------\n",
        "# Query helper (builds url-encoded OData filter)\n",
        "# -------------------------\n",
        "def query_catalog(filter_expr, token, timeout=60):\n",
        "    q = {\n",
        "        \"$filter\": filter_expr,\n",
        "        \"$top\": \"1\",\n",
        "        \"$orderby\": \"ContentDate/Start desc\"\n",
        "    }\n",
        "    # build URL with encoded query string\n",
        "    query_str = \"&\".join(f\"{k}={urllib.parse.quote(v, safe='')}\" for k, v in q.items())\n",
        "    url = f\"{CATALOG_URL}?{query_str}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "    # caller handles 401/403 by reauthenticating (we reauth every loop anyway)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n",
        "\n",
        "# -------------------------\n",
        "# Helper: clean extracted files\n",
        "# -------------------------\n",
        "def cleanup_data_dir():\n",
        "    import shutil\n",
        "    for p in glob.glob(os.path.join(WORK_DIR, \"*\")):\n",
        "        try:\n",
        "            if os.path.isdir(p):\n",
        "                shutil.rmtree(p)\n",
        "            else:\n",
        "                os.remove(p)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# -------------------------\n",
        "# Load and prepare monthly list (keeps your original logic, fixed for Period/Timestamp)\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "df = pd.read_csv(DRIVE_CSV_IN)\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "df = df.dropna(subset=['date'])\n",
        "df = df[df['date'] >= \"2015-06-23\"]\n",
        "\n",
        "monthly_df = df.groupby(df['date'].dt.to_period('M'), as_index=False).first().rename(columns={'date': 'month_period'})\n",
        "monthly_df['date'] = monthly_df['month_period'].apply(lambda x: x.to_timestamp() if hasattr(x, \"to_timestamp\") else pd.Timestamp(x))\n",
        "monthly_df.drop(columns=['month_period'], inplace=True)\n",
        "\n",
        "# -------------------------\n",
        "# Main loop: re-auth each iteration (as you requested)\n",
        "# -------------------------\n",
        "results = []\n",
        "total = len(monthly_df)\n",
        "for idx, row in tqdm(monthly_df.iterrows(), total=total, desc=\"Months\"):\n",
        "    date = row['date']\n",
        "    # re-authenticate every iteration (explicitly)\n",
        "    try:\n",
        "        token = authenticate()\n",
        "    except Exception as e:\n",
        "        print(f\"Auth failed at {date}: {e}\")\n",
        "        # append NaN row and continue\n",
        "        nan_feat = [np.nan] * 768\n",
        "        rec = {\"date\": date}\n",
        "        rec.update({f\"feat_{i}\": float(\"nan\") for i in range(len(nan_feat))})\n",
        "        results.append(rec)\n",
        "        continue\n",
        "\n",
        "    start_date = (date - pd.Timedelta(days=7)).strftime(\"%Y-%m-%dT00:00:00.000Z\")\n",
        "    end_date   = (date + pd.Timedelta(days=7)).strftime(\"%Y-%m-%dT23:59:59.000Z\")\n",
        "\n",
        "    filter_expr = (\n",
        "        \"Collection/Name eq 'SENTINEL-2' and \"\n",
        "        \"Attributes/OData.CSC.StringAttribute/any(a: a/Name eq 'productType' and a/Value eq 'S2MSI2A') and \"\n",
        "        \"Attributes/OData.CSC.DoubleAttribute/any(a: a/Name eq 'cloudCover' and a/Value lt 15) and \"\n",
        "        f\"ContentDate/Start gt {start_date} and ContentDate/Start lt {end_date}\"\n",
        "    )\n",
        "\n",
        "    feat = np.full(768, np.nan)  # default\n",
        "\n",
        "    try:\n",
        "        resp_json = query_catalog(filter_expr, token)\n",
        "        items = resp_json.get(\"value\", [])\n",
        "        if not items:\n",
        "            # No product for this month: keep NaN vector\n",
        "            print(f\"No Sentinel-2 item found for {date}\")\n",
        "            rec = {\"date\": date}\n",
        "            rec.update({f\"feat_{i}\": float(\"nan\") for i in range(len(feat))})\n",
        "            results.append(rec)\n",
        "            # polite delay\n",
        "            time.sleep(1.0)\n",
        "            cleanup_data_dir()\n",
        "            continue\n",
        "\n",
        "        product = items[0]\n",
        "        pid = product[\"Id\"]\n",
        "        pname = product[\"Name\"]\n",
        "        zip_path = os.path.join(WORK_DIR, f\"{pname}.zip\")\n",
        "\n",
        "        # Download product zip (reauth handled above)\n",
        "        dl_url = f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({pid})/$value\"\n",
        "        with requests.get(dl_url, headers={\"Authorization\": f\"Bearer {token}\"}, stream=True, timeout=120) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(zip_path, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "\n",
        "        # Extract zip to WORK_DIR\n",
        "        with ZipFile(zip_path, \"r\") as z:\n",
        "            z.extractall(WORK_DIR)\n",
        "\n",
        "        # find SAFE directory\n",
        "        safe_dirs = [d for d in os.listdir(WORK_DIR) if d.endswith(\".SAFE\")]\n",
        "        if not safe_dirs:\n",
        "            raise ValueError(\"No .SAFE directory found after extraction\")\n",
        "        safe_dir = safe_dirs[0]\n",
        "\n",
        "        # find 10m RGB bands (B02,B03,B04) — pattern used earlier\n",
        "        band_paths = glob.glob(os.path.join(WORK_DIR, safe_dir, \"GRANULE\", \"*\", \"IMG_DATA\", \"R10m\", \"*B0[234]_10m.jp2\"))\n",
        "        band_paths.sort()\n",
        "        if len(band_paths) < 3:\n",
        "            raise ValueError(\"Incomplete RGB band files (expected 3). Found: {}\".format(len(band_paths)))\n",
        "\n",
        "        # Read bands\n",
        "        bands = []\n",
        "        for b in band_paths:\n",
        "            with rasterio.open(b) as src:\n",
        "                arr = src.read(1).astype(np.float32)\n",
        "                # treat all-NaN or all-zero as invalid\n",
        "                if np.all(np.isnan(arr)) or np.all(arr == 0):\n",
        "                    raise ValueError(f\"Invalid band (all NaN or zero): {b}\")\n",
        "                bands.append(arr)\n",
        "\n",
        "        # Stack into RGB (ensure correct order: depending on file names order might be B02,B03,B04)\n",
        "        # band_paths sorted should align with B02,B03,B04; we will stack as (B04,B03,B02) => R,G,B if necessary.\n",
        "        # To be safe, inspect names and pick according to B0X substring:\n",
        "        def band_index(path):\n",
        "            basename = os.path.basename(path)\n",
        "            if \"B02\" in basename: return 2\n",
        "            if \"B03\" in basename: return 3\n",
        "            if \"B04\" in basename: return 4\n",
        "            return 0\n",
        "        ordered = sorted(band_paths, key=band_index)\n",
        "        # Now read ordered arrays\n",
        "        bands = []\n",
        "        for b in ordered:\n",
        "            with rasterio.open(b) as src:\n",
        "                arr = src.read(1).astype(np.float32)\n",
        "                bands.append(arr)\n",
        "\n",
        "        # Determine mapping to R,G,B: B04->R, B03->G, B02->B\n",
        "        # Create dict mapping\n",
        "        band_map = {}\n",
        "        for p in ordered:\n",
        "            name = os.path.basename(p)\n",
        "            if \"B02\" in name: band_map['B02'] = p\n",
        "            if \"B03\" in name: band_map['B03'] = p\n",
        "            if \"B04\" in name: band_map['B04'] = p\n",
        "\n",
        "        # read arrays in correct R,G,B order\n",
        "        with rasterio.open(band_map['B04']) as sR:\n",
        "            R = sR.read(1).astype(np.float32)\n",
        "        with rasterio.open(band_map['B03']) as sG:\n",
        "            G = sG.read(1).astype(np.float32)\n",
        "        with rasterio.open(band_map['B02']) as sB:\n",
        "            B = sB.read(1).astype(np.float32)\n",
        "\n",
        "        rgb = np.stack([R, G, B], axis=-1)  # H x W x 3\n",
        "\n",
        "        # replace NaNs -> 0\n",
        "        rgb = np.nan_to_num(rgb, nan=0.0)\n",
        "\n",
        "        # robust normalization:\n",
        "        upper = np.nanpercentile(rgb, 99)\n",
        "        if (upper == 0) or np.isnan(upper):\n",
        "            upper = np.nanmax(rgb)\n",
        "        if (upper == 0) or np.isnan(upper):\n",
        "            raise ValueError(\"Image is all zeros / cannot normalize\")\n",
        "\n",
        "        rgb = np.clip(rgb / float(upper), 0.0, 1.0)\n",
        "\n",
        "        # resize to 224x224 (ViT input)\n",
        "        rgb_resized = cv2.resize(rgb, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Convert to uint8 0-255 to avoid extractor rescale warnings (extractor expects 0-255 by default)\n",
        "        img_uint8 = (np.clip(rgb_resized, 0.0, 1.0) * 255.0).round().astype(np.uint8)\n",
        "\n",
        "        # Feed extractor / model (move tensors to device)\n",
        "        inputs = extractor(images=img_uint8, return_tensors=\"pt\", do_resize=False, do_rescale=False)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        # feature vector\n",
        "        feat = outputs.pooler_output.cpu().numpy().flatten()\n",
        "\n",
        "        # convert to Python floats for CSV\n",
        "        feat = feat.astype(float)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {date}: {e}\")\n",
        "        feat = np.full(768, np.nan)\n",
        "\n",
        "    finally:\n",
        "        # cleanup files extracted each iteration to conserve disk\n",
        "        cleanup_data_dir()\n",
        "\n",
        "    # append record with date and features\n",
        "    rec = {\"date\": date}\n",
        "    rec.update({f\"feat_{i}\": (float(v) if (not np.isnan(v)) else float(\"nan\")) for i, v in enumerate(feat)})\n",
        "    results.append(rec)\n",
        "\n",
        "    # polite small delay\n",
        "    time.sleep(1.0)\n",
        "\n",
        "# Save results\n",
        "out_df = pd.DataFrame(results)\n",
        "out_df.to_csv(DRIVE_OUTPUT, index=False)\n",
        "print(\"Saved features to:\", DRIVE_OUTPUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i30ho21pk9yy",
        "outputId": "6c565c62-d4a9-431a-d018-92a3e13551cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/vit/feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rMonths:   0%|          | 0/125 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Sentinel-2 item found for 2015-06-23 00:00:00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Months:  35%|███▌      | 44/125 [1:38:59<1:43:43, 76.83s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing 2019-02-01 00:00:00: Invalid band (all NaN or zero): data/S2B_MSIL2A_20190208T235259_N0500_R030_T57PZK_20221126T103409.SAFE/GRANULE/L2A_T57PZK_A010066_20190208T235300/IMG_DATA/R10m/T57PZK_20190208T235259_B02_10m.jp2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Months:  97%|█████████▋| 121/125 [5:13:55<11:57, 179.31s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}